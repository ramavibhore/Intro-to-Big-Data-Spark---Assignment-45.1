[acadgild@localhost ~]$ spark-shell
2018-09-24 17:29:57 WARN  Utils:66 - Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 10.0.2.15 instead (on interface eth1)
2018-09-24 17:29:57 WARN  Utils:66 - Set SPARK_LOCAL_IP if you need to bind to another address
2018-09-24 17:29:59 WARN  NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
Spark context Web UI available at http://10.0.2.15:4040
Spark context available as 'sc' (master = local[*], app id = local-1537790424776).
Spark session available as 'spark'.
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 2.3.0
      /_/
         
Using Scala version 2.11.8 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_161)
Type in expressions to have them evaluated.
Type :help for more information.

Task 3
1. Write a program to read a text file and print the number of rows of data in the document.

scala> val textFile = sc.textFile("file:///home/acadgild/Desktop/SampleDoc")
textFile: org.apache.spark.rdd.RDD[String] = file:///home/acadgild/Desktop/SampleDoc MapPartitionsRDD[1] at textFile at <console>:24

scala> textFile.count
res0: Long = 3                                                                  

scala> textFile.
++                      getCheckpointFile        reduce             
aggregate               getNumPartitions         repartition        
cache                   getStorageLevel          sample             
canEqual                glom                     saveAsObjectFile   
cartesian               groupBy                  saveAsTextFile     
checkpoint              id                       setName            
coalesce                intersection             sortBy             
collect                 isCheckpointed           sparkContext       
collectAsync            isEmpty                  subtract           
compute                 iterator                 take               
context                 keyBy                    takeAsync          
copy                    localCheckpoint          takeOrdered        
count                   map                      takeSample         
countApprox             mapPartitions            toDF               
countApproxDistinct     mapPartitionsWithIndex   toDS               
countAsync              max                      toDebugString      
countByValue            min                      toJavaRDD          
countByValueApprox      name                     toLocalIterator    
dependencies            partitioner              toString           
distinct                partitions               top                
filter                  persist                  treeAggregate      
first                   pipe                     treeReduce         
flatMap                 preferredLocations       union              
fold                    productArity             unpersist          
foreach                 productElement           zip                
foreachAsync            productIterator          zipPartitions      
foreachPartition        productPrefix            zipWithIndex       
foreachPartitionAsync   randomSplit              zipWithUniqueId    

scala> textFile.collect
res1: Array[String] = Array(This-is-my-first-assignment., It-will-count-the-number-of-lines-in-this-document., The-total-number-of-lines-is-3)

scala> textFile.first
res2: String = This-is-my-first-assignment.

2. Write a program to read a text file and print the number of words in the document.

scala> val lines = textFile.flatMap(line => line.split(" "))
lines: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[2] at flatMap at <console>:25

scala> lines.collect
res3: Array[String] = Array(This-is-my-first-assignment., It-will-count-the-number-of-lines-in-this-document., The-total-number-of-lines-is-3)

scala> val numberOfWords = lines.count
numberOfWords: Long = 3

scala> numberOfWords
res5: Long = 3

3. We have a document where the word separator is -, instead of space. Write a spark code, to obtain the count of the total number of words present in the document.

scala> val words = textFile.flatMap(line => line.split("-"))
words: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[3] at flatMap at <console>:25

scala> words.collect
res6: Array[String] = Array(This, is, my, first, assignment., It, will, count, the, number, of, lines, in, this, document., The, total, number, of, lines, is, 3)

scala> val no_ofWordWithSeparator = words.count
no_ofWordWithSeparator: Long = 22

scala> no_ofWordWithSeparator
res7: Long = 22


